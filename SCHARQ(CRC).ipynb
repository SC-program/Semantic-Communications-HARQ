{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f205321b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 30)]              0         \n",
      "                                                                 \n",
      " positional_embedding_3 (Po  (None, 30, 128)           643840    \n",
      " sitionalEmbedding)                                              \n",
      "                                                                 \n",
      " transformer_block_6 (Trans  (None, 30, 128)           561024    \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " transformer_block_7 (Trans  (None, 30, 128)           561024    \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " transformer_block_8 (Trans  (None, 30, 128)           561024    \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 3840)              0         \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 250)               960250    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3287162 (12.54 MB)\n",
      "Trainable params: 3287162 (12.54 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_8 (InputLayer)        [(None, 250)]                0         []                            \n",
      "                                                                                                  \n",
      " tf.reshape_1 (TFOpLambda)   (None, 250)                  0         ['input_8[0][0]']             \n",
      "                                                                                                  \n",
      " dense_29 (Dense)            (None, 3840)                 963840    ['tf.reshape_1[0][0]']        \n",
      "                                                                                                  \n",
      " input_7 (InputLayer)        [(None, 30)]                 0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_28 (La  (None, 3840)                 7680      ['dense_29[0][0]']            \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " positional_embedding_4 (Po  (None, 30, 128)              643840    ['input_7[0][0]']             \n",
      " sitionalEmbedding)                                                                               \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)         (None, 30, 128)              0         ['layer_normalization_28[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " transformer_decoder_3 (Tra  (None, 30, 128)              1088768   ['positional_embedding_4[0][0]\n",
      " nsformerDecoder)                                                   ',                            \n",
      "                                                                     'reshape_1[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)        (None, 30, 128)              0         ['transformer_decoder_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " transformer_decoder_4 (Tra  (None, 30, 128)              1088768   ['dropout_21[0][0]',          \n",
      " nsformerDecoder)                                                    'reshape_1[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)        (None, 30, 128)              0         ['transformer_decoder_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " transformer_decoder_5 (Tra  (None, 30, 128)              1088768   ['dropout_22[0][0]',          \n",
      " nsformerDecoder)                                                    'reshape_1[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)        (None, 30, 128)              0         ['transformer_decoder_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dense_36 (Dense)            (None, 30, 5000)             645000    ['dropout_23[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5526664 (21.08 MB)\n",
      "Trainable params: 5526664 (21.08 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"sequence_length\": self.sequence_length,\n",
    "                \"vocab_size\": self.vocab_size,\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim,  num_heads,ff_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = ff_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim,\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.add = layers.Add()  # instead of `+` to preserve mask\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs):\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs, value=inputs, key=inputs, use_causal_mask=True\n",
    "        )\n",
    "        out_1 = self.layernorm_1(self.add([inputs, attention_output_1]))\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(self.add([out_1, attention_output_2]))\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(self.add([out_2, proj_output]))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "                \"latent_dim\": self.latent_dim,\n",
    "                \"num_heads\": self.num_heads,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "maxlen=30\n",
    "embed_dim = 128  # Embedding size for each token\n",
    "num_heads = 8  # Number of attention heads\n",
    "ff_dim = 128  # Hidden layer size in feed forward network inside transformer\n",
    "block_num=3 # block_num=6 in the paper\n",
    "vocab_size=5000\n",
    "num_bits=500\n",
    "B=2\n",
    "def SC_encoder(inputs,flat_flag=True):\n",
    "    embedding_layer = PositionalEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "   \n",
    "    for i in range(block_num):\n",
    "        x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)\n",
    "    if flat_flag:\n",
    "        x=layers.Flatten()(x)\n",
    "    x=layers.Dense(num_bits//B,activation='sigmoid')(x)\n",
    "    return x\n",
    "def SC_decoder(decoder_inputs,encoded_seq_inputs,flat_flag=True): \n",
    "    y=tf.reshape(encoded_seq_inputs,[-1,num_bits//B])\n",
    "    y=layers.Dense(embed_dim*maxlen)(y)\n",
    "    y=layers.LayerNormalization(epsilon=1e-6)(y)\n",
    "    y=layers.Reshape((maxlen, embed_dim))(y)\n",
    "    x = PositionalEmbedding(maxlen, vocab_size, embed_dim)(decoder_inputs)\n",
    "    for i in range(block_num):\n",
    "        x = TransformerDecoder(embed_dim, num_heads, ff_dim)(x, y)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "    decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "    return decoder_outputs\n",
    "encoder_inputs = keras.Input(shape=(maxlen), dtype=\"int64\")\n",
    "encoder_outputs = SC_encoder(encoder_inputs)\n",
    "encoder0 = keras.Model(encoder_inputs, encoder_outputs)\n",
    "encoder0.summary()\n",
    "decoder_inputs = keras.Input(shape=(maxlen), dtype=\"int64\")\n",
    "encoded_seq_inputs = keras.Input(shape=( num_bits//B))\n",
    "decoder_outputs = SC_decoder(decoder_inputs, encoded_seq_inputs)\n",
    "decoder0 = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
    "decoder0.summary() \n",
    "decoder_outputs = decoder0([decoder_inputs, encoder_outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dac58ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(667875, 30)\n",
      "None\n",
      "Model: \"SC_en_de\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)        [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " tf.ones_like_1 (TFOpLambda  (None, None)                 0         ['input_9[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " model_3 (Functional)        (None, 250)                  3287162   ['input_9[0][0]']             \n",
      "                                                                                                  \n",
      " model_4 (Functional)        (None, 30, 5000)             5526664   ['tf.ones_like_1[0][0]',      \n",
      "                                                                     'model_3[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8813826 (33.62 MB)\n",
      "Trainable params: 8813826 (33.62 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from europarl import TokenizerWrap\n",
    "with open('tokenizer_5000_1.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "encoder_input_data=tokenizer.tokens_padded\n",
    "print(print(encoder_input_data.shape))\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "encoder_outputs=encoder0(encoder_inputs)\n",
    "decoder_inputs = tf.ones_like(encoder_inputs, dtype=\"int64\")\n",
    "decoder_outputs=decoder0([decoder_inputs,encoder_outputs])\n",
    "SC_en_de = keras.Model(\n",
    "    encoder_inputs, decoder_outputs, name=\"SC_en_de\"\n",
    ")\n",
    "SC_en_de.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fc37e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# X = tokenizer.tokens_padded[0:500000]\n",
    "# Y = tokenizer.tokens_padded[500000:]\n",
    "filepath ='SC/'\n",
    "if not os.path.exists(filepath):\n",
    "                os.mkdir(filepath[:-1])\n",
    "if not os.path.exists(filepath+'/his'):\n",
    "                os.mkdir(filepath)\n",
    "# class logging(keras.callbacks.Callback):\n",
    "#     def __init__(self, model, path='SC.txt'):\n",
    "#         super(logging, self).__init__()\n",
    "\n",
    "#         self.epochs_since_last_save = 0\n",
    "#         self.model = model\n",
    "#         self.path = path\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs={}):\n",
    "#         if (epoch == 0):\n",
    "#             with open('SC/his/' + self.path, 'w') as f:\n",
    "#                 # f.write(str(np.shape(HH))+'\\n')\n",
    "#                 f.write('Begin~' + '\\n')\n",
    "#         with open('SC/his/' + self.path, 'a') as f:\n",
    "#             # f.write(str(np.shape(HH))+'\\n')\n",
    "#             f.write(str(logs.get('loss'))+ '\\t'+str(logs.get('val_accuracy')) + '\\n')\n",
    "#        # print('SNR:',random_SNR(10))\n",
    "#         if (epoch%10==0):\n",
    "#             encoder0.save_weights(filepath+'SC_en_'+str(epoch)+'.h5')\n",
    "#             decoder0.save_weights(filepath+'SC_de_'+str(epoch)+'.h5')\n",
    "# epochs = 100  # SC_en+SC_de step1\n",
    "# SC_en_de.compile(\n",
    "#     optimizer=keras.optimizers.Adam(learning_rate=0.0001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "# )\n",
    "# log=logging(SC_en_de)\n",
    "# SC_en_de.fit(X,X,batch_size=128, epochs=epochs, validation_data=(Y,Y),callbacks=[log])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3895276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder0.save_weights(filepath+'SC_en0.h5')\n",
    "# decoder0.save_weights(filepath+'SC_de0.h5')  \n",
    "encoder0.load_weights(filepath+'SC_en0.h5')\n",
    "decoder0.load_weights(filepath+'SC_de0.h5') \n",
    "encoder0.trainable=False\n",
    "decoder0.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "911c0cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_21 (InputLayer)       [(None, 30)]              0         \n",
      "                                                                 \n",
      " positional_embedding_13 (P  (None, 30, 128)           643840    \n",
      " ositionalEmbedding)                                             \n",
      "                                                                 \n",
      " transformer_block_24 (Tran  (None, 30, 128)           561024    \n",
      " sformerBlock)                                                   \n",
      "                                                                 \n",
      " transformer_block_25 (Tran  (None, 30, 128)           561024    \n",
      " sformerBlock)                                                   \n",
      "                                                                 \n",
      " transformer_block_26 (Tran  (None, 30, 128)           561024    \n",
      " sformerBlock)                                                   \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 3840)              0         \n",
      "                                                                 \n",
      " dense_102 (Dense)           (None, 250)               960250    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3287162 (12.54 MB)\n",
      "Trainable params: 3287162 (12.54 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_23 (InputLayer)       [(None, 250)]                0         []                            \n",
      "                                                                                                  \n",
      " tf.reshape_5 (TFOpLambda)   (None, 250)                  0         ['input_23[0][0]']            \n",
      "                                                                                                  \n",
      " dense_103 (Dense)           (None, 3840)                 963840    ['tf.reshape_5[0][0]']        \n",
      "                                                                                                  \n",
      " input_22 (InputLayer)       [(None, 30)]                 0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_104 (L  (None, 3840)                 7680      ['dense_103[0][0]']           \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " positional_embedding_14 (P  (None, 30, 128)              643840    ['input_22[0][0]']            \n",
      " ositionalEmbedding)                                                                              \n",
      "                                                                                                  \n",
      " reshape_5 (Reshape)         (None, 30, 128)              0         ['layer_normalization_104[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " transformer_decoder_15 (Tr  (None, 30, 128)              1088768   ['positional_embedding_14[0][0\n",
      " ansformerDecoder)                                                  ]',                           \n",
      "                                                                     'reshape_5[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_69 (Dropout)        (None, 30, 128)              0         ['transformer_decoder_15[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " transformer_decoder_16 (Tr  (None, 30, 128)              1088768   ['dropout_69[0][0]',          \n",
      " ansformerDecoder)                                                   'reshape_5[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_70 (Dropout)        (None, 30, 128)              0         ['transformer_decoder_16[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " transformer_decoder_17 (Tr  (None, 30, 128)              1088768   ['dropout_70[0][0]',          \n",
      " ansformerDecoder)                                                   'reshape_5[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_71 (Dropout)        (None, 30, 128)              0         ['transformer_decoder_17[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_110 (Dense)           (None, 30, 5000)             645000    ['dropout_71[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5526664 (21.08 MB)\n",
      "Trainable params: 5526664 (21.08 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_bits1=400\n",
    "encoder_inputs = keras.Input(shape=(maxlen), dtype=\"int64\")\n",
    "encoder_outputs = SC_encoder(encoder_inputs)\n",
    "encoder1 = keras.Model(encoder_inputs, encoder_outputs)\n",
    "encoder1.summary()\n",
    "decoder_inputs = keras.Input(shape=(maxlen), dtype=\"int64\")\n",
    "encoded_seq_inputs = keras.Input(shape=( num_bits//B))\n",
    "decoder_outputs = SC_decoder(decoder_inputs, encoded_seq_inputs)\n",
    "decoder1 = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
    "decoder1.summary() \n",
    "decoder_outputs = decoder0([decoder_inputs, encoder_outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ed82de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder1.load_weights(filepath+'SC_en0.h5')\n",
    "decoder1.load_weights(filepath+'SC_de0.h5') \n",
    "encoder1.trainable=False\n",
    "decoder1.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fcba1c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "B=2\n",
    "N=16\n",
    "d_model=128\n",
    "\n",
    "def Num2Bit(Num, B):\n",
    "    Num_ = Num.numpy()\n",
    "    bit = (np.unpackbits(np.array(Num_, np.uint8), axis=1).reshape(-1, Num_.shape[1], 8)[:, :, (8-B):]).reshape(-1,\n",
    "                                                                                                            Num_.shape[\n",
    "                                                                                                                1] * B)\n",
    "    bit.astype(np.float32)\n",
    "    return tf.convert_to_tensor(bit, dtype=tf.float32)\n",
    "# Bit to Number Function Defining\n",
    "def Bit2Num(Bit, B):\n",
    "    Bit_ = Bit.numpy()\n",
    "    Bit_.astype(np.float32)\n",
    "    Bit_ = np.reshape(Bit_, [-1, int(Bit_.shape[1] / B), B])\n",
    "    num = np.zeros(shape=np.shape(Bit_[:, :, 1]))\n",
    "    for i in range(B):\n",
    "        num = num + Bit_[:, :, i] * 2 ** (B - 1 - i)\n",
    "    return tf.cast(num, dtype=tf.float32)\n",
    "#=======================================================================================================================\n",
    "#=======================================================================================================================\n",
    "# Quantization and Dequantization Layers Defining\n",
    "@tf.custom_gradient\n",
    "def QuantizationOp(x, B,num_bits):\n",
    "    step = tf.cast((2 ** B), dtype=tf.float32)\n",
    "    result = tf.cast((tf.round(x * step - 0.5)), dtype=tf.float32)\n",
    "    dim = result.shape[1]\n",
    "    result = tf.py_function(func=Num2Bit, inp=[result, B], Tout=tf.float32)\n",
    "    result = tf.reshape(result, [-1, num_bits])\n",
    "    def custom_grad(dy):\n",
    "        grad = dy\n",
    "        return (grad, grad,grad)\n",
    "    return result, custom_grad\n",
    "class QuantizationLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, B,num_bits,**kwargs):\n",
    "        self.B = B\n",
    "        self.num_bits = num_bits\n",
    "        super(QuantizationLayer, self).__init__()\n",
    "    def call(self, x):\n",
    "        return QuantizationOp(x, self.B, self.num_bits)\n",
    "    def get_config(self):\n",
    "        # Implement get_config to enable serialization. This is optional.\n",
    "        base_config = super(QuantizationLayer, self).get_config()\n",
    "        base_config['B'] = self.B\n",
    "        return base_config\n",
    "@tf.custom_gradient\n",
    "def DequantizationOp(x, B,num_bits):\n",
    "    dim = x.shape[1]\n",
    "    x = tf.py_function(func=Bit2Num, inp=[x, B], Tout=tf.float32)\n",
    "    x = tf.reshape(x, (-1, num_bits//B))\n",
    "    step = tf.cast((2 ** B), dtype=tf.float32)\n",
    "    result = tf.cast((x + 0.5) / step, dtype=tf.float32)\n",
    "    def custom_grad(dy):\n",
    "        grad = dy * 1\n",
    "        return (grad, grad,grad)\n",
    "    return result, custom_grad\n",
    "class DeuantizationLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, B,num_bits,**kwargs):\n",
    "        self.B = B\n",
    "        self.num_bits=num_bits\n",
    "        super(DeuantizationLayer, self).__init__()\n",
    "    def call(self, x):\n",
    "        return DequantizationOp(x, self.B,self.num_bits)\n",
    "    def get_config(self):\n",
    "        base_config = super(DeuantizationLayer, self).get_config()\n",
    "        base_config['B'] = self.B\n",
    "        return base_config\n",
    "@tf.custom_gradient\n",
    "def BSCOp(bits, rate):\n",
    "    errorbits = tf.keras.backend.random_binomial(shape=tf.shape(bits), p=rate)\n",
    "    result=tf.reshape(tf.math.floormod(bits+errorbits,2),tf.shape(bits))\n",
    "    def custom_grad(dy):\n",
    "        grad = dy * 1\n",
    "        return (grad, grad)\n",
    "    return result, custom_grad\n",
    "class BSC(tf.keras.layers.Layer):\n",
    "    def __init__(self, rate, **kwargs): #**kwargs```TypeError: __init__() got an unexpected keyword argument 'name'```\n",
    "        self.rate = rate\n",
    "        super(BSC, self).__init__()\n",
    "    def call(self, bits):\n",
    "        \n",
    "        return BSCOp(bits,self.rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0eae929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_37\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_56 (InputLayer)       [(None, 250)]             0         \n",
      "                                                                 \n",
      " dense_135 (Dense)           (None, 250)               62750     \n",
      "                                                                 \n",
      " quantization_layer_16 (Qua  (None, 500)               0         \n",
      " ntizationLayer)                                                 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 62750 (245.12 KB)\n",
      "Trainable params: 62750 (245.12 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"model_38\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_57 (InputLayer)       [(None, 250)]             0         \n",
      "                                                                 \n",
      " dense_136 (Dense)           (None, 200)               50200     \n",
      "                                                                 \n",
      " quantization_layer_17 (Qua  (None, 400)               0         \n",
      " ntizationLayer)                                                 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50200 (196.09 KB)\n",
      "Trainable params: 50200 (196.09 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "(None, 250)\n",
      "(None, 200)\n",
      "Model: \"model_39\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_58 (InputLayer)       [(None, 250)]             0         \n",
      "                                                                 \n",
      " dense_137 (Dense)           (None, 250)               62750     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 62750 (245.12 KB)\n",
      "Trainable params: 62750 (245.12 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"SC_en_Q_deQ_SC_de0\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_55 (InputLayer)       [(None, 30)]                 0         []                            \n",
      "                                                                                                  \n",
      " model_10 (Functional)       (None, 250)                  3287162   ['input_55[0][0]']            \n",
      "                                                                                                  \n",
      " dense_135 (Dense)           (None, 250)                  62750     ['model_10[8][0]']            \n",
      "                                                                                                  \n",
      " quantization_layer_16 (Qua  (None, 500)                  0         ['dense_135[0][0]']           \n",
      " ntizationLayer)                                                                                  \n",
      "                                                                                                  \n",
      " bsc_16 (BSC)                (None, 500)                  0         ['quantization_layer_16[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " deuantization_layer_16 (De  (None, 250)                  0         ['bsc_16[0][0]']              \n",
      " uantizationLayer)                                                                                \n",
      "                                                                                                  \n",
      " tf.ones_like_10 (TFOpLambd  (None, 30)                   0         ['input_55[0][0]']            \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " dense_137 (Dense)           (None, 250)                  62750     ['deuantization_layer_16[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " model_11 (Functional)       (None, 30, 5000)             5526664   ['tf.ones_like_10[0][0]',     \n",
      "                                                                     'dense_137[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8939326 (34.10 MB)\n",
      "Trainable params: 0 (0.00 Byte)\n",
      "Non-trainable params: 8939326 (34.10 MB)\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_40\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_59 (InputLayer)       [(None, 450)]             0         \n",
      "                                                                 \n",
      " dense_138 (Dense)           (None, 250)               112750    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 112750 (440.43 KB)\n",
      "Trainable params: 112750 (440.43 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"SC_en_Q_deQ_SC_de1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_55 (InputLayer)       [(None, 30)]                 0         []                            \n",
      "                                                                                                  \n",
      " model_10 (Functional)       (None, 250)                  3287162   ['input_55[0][0]']            \n",
      "                                                                                                  \n",
      " model_12 (Functional)       (None, 250)                  3287162   ['input_55[0][0]']            \n",
      "                                                                                                  \n",
      " dense_135 (Dense)           (None, 250)                  62750     ['model_10[8][0]']            \n",
      "                                                                                                  \n",
      " dense_136 (Dense)           (None, 200)                  50200     ['model_12[8][0]']            \n",
      "                                                                                                  \n",
      " quantization_layer_16 (Qua  (None, 500)                  0         ['dense_135[0][0]']           \n",
      " ntizationLayer)                                                                                  \n",
      "                                                                                                  \n",
      " quantization_layer_17 (Qua  (None, 400)                  0         ['dense_136[0][0]']           \n",
      " ntizationLayer)                                                                                  \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " bsc_16 (BSC)                (None, 500)                  0         ['quantization_layer_16[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " bsc_17 (BSC)                (None, 400)                  0         ['quantization_layer_17[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " deuantization_layer_16 (De  (None, 250)                  0         ['bsc_16[0][0]']              \n",
      " uantizationLayer)                                                                                \n",
      "                                                                                                  \n",
      " deuantization_layer_17 (De  (None, 200)                  0         ['bsc_17[0][0]']              \n",
      " uantizationLayer)                                                                                \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate  (None, 450)                  0         ['deuantization_layer_16[0][0]\n",
      " )                                                                  ',                            \n",
      "                                                                     'deuantization_layer_17[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.ones_like_11 (TFOpLambd  (None, 30)                   0         ['input_55[0][0]']            \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " dense_138 (Dense)           (None, 250)                  112750    ['concatenate_8[0][0]']       \n",
      "                                                                                                  \n",
      " model_13 (Functional)       (None, 30, 5000)             5526664   ['tf.ones_like_11[0][0]',     \n",
      "                                                                     'dense_138[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 12326688 (47.02 MB)\n",
      "Trainable params: 8976776 (34.24 MB)\n",
      "Non-trainable params: 3349912 (12.78 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = keras.Input(shape=(30,), dtype=\"int64\")\n",
    "SC_en_codeword0 = encoder0(encoder_inputs)\n",
    "Q_input0=layers.Dense(num_bits//B,activation='sigmoid')(SC_en_codeword0)\n",
    "bits0=QuantizationLayer(B,num_bits)(Q_input0)#transmission bits\n",
    "Qmodel0=keras.Model(SC_en_codeword0,bits0)\n",
    "Qmodel0.summary()\n",
    "###############\n",
    "SC_en_codeword1 = encoder1(encoder_inputs)\n",
    "Q_input1=layers.Dense(num_bits1//B,activation='sigmoid')(SC_en_codeword1)\n",
    "bits1=QuantizationLayer(B,num_bits1)(Q_input1)#transmission bits\n",
    "Qmodel1=keras.Model(SC_en_codeword1,bits1)\n",
    "Qmodel1.summary()\n",
    "###############\n",
    "rx_bits0=BSC(0.05)(bits0)\n",
    "rx_bits1=BSC(0.05)(bits1)\n",
    "################concat b0 b1#########\n",
    "Q_output0=DeuantizationLayer(B,num_bits)(rx_bits0)\n",
    "Q_output1=DeuantizationLayer(B,num_bits1)(rx_bits1)\n",
    "print(Q_output0.shape)\n",
    "print(Q_output1.shape)\n",
    "Q_output=layers.Concatenate()([Q_output0,Q_output1])\n",
    "########################decoder0##########\n",
    "SC_de_codeword0=layers.Dense(num_bits//B,activation='sigmoid')(Q_output0)\n",
    "deQmodel0=keras.Model(Q_output0,SC_de_codeword0)\n",
    "deQmodel0.summary()\n",
    "decoder_inputs = tf.ones_like(encoder_inputs, dtype=\"int64\")\n",
    "decoder_outputs=decoder0([decoder_inputs,SC_de_codeword0])\n",
    "Qmodel0.load_weights(filepath+'Q_model_90.h5')\n",
    "deQmodel0.load_weights(filepath+'deQ_model_90.h5')\n",
    "Qmodel0.trainable=False\n",
    "deQmodel0.trainable=False\n",
    "SC_en_Q_deQ_SC_de0 = keras.Model(\n",
    "    encoder_inputs, decoder_outputs, name=\"SC_en_Q_deQ_SC_de0\"\n",
    ")\n",
    "SC_en_Q_deQ_SC_de0.summary()\n",
    "\n",
    "#############decoder1########\n",
    "SC_de_codeword1=layers.Dense(num_bits//B,activation='sigmoid')(Q_output)\n",
    "deQmodel1=keras.Model(Q_output,SC_de_codeword1)\n",
    "deQmodel1.summary()\n",
    "decoder_inputs = tf.ones_like(encoder_inputs, dtype=\"int64\")\n",
    "decoder_outputs=decoder1([decoder_inputs,SC_de_codeword1])\n",
    "\n",
    "SC_en_Q_deQ_SC_de1 = keras.Model(\n",
    "    encoder_inputs, decoder_outputs, name=\"SC_en_Q_deQ_SC_de1\"\n",
    ")\n",
    "SC_en_Q_deQ_SC_de1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "52d1d073",
   "metadata": {},
   "outputs": [],
   "source": [
    "SC_en_Q_deQ_SC_de0.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "SC_en_Q_deQ_SC_de1.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e44f370e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 3s 22ms/step - loss: 2.0829 - accuracy: 0.7423\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.0829012393951416, 0.7423333525657654]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SC_en_Q_deQ_SC_de0.evaluate(Y[:100],Y[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9490ef07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1292/3907 [========>.....................] - ETA: 3:55 - loss: 1.3766 - accuracy: 0.7395"
     ]
    }
   ],
   "source": [
    "encoder1.trainable=True\n",
    "decoder1.trainable=True\n",
    "X = tokenizer.tokens_padded[0:500000]\n",
    "Y = tokenizer.tokens_padded[500000:]\n",
    "filepath ='SC/'\n",
    "if not os.path.exists(filepath):\n",
    "                os.mkdir(filepath[:-1])\n",
    "if not os.path.exists(filepath+'/his'):\n",
    "                os.mkdir(filepath)\n",
    "class logging(keras.callbacks.Callback):\n",
    "    def __init__(self, model, path='Q_SC.txt'):\n",
    "        super(logging, self).__init__()\n",
    "\n",
    "        self.epochs_since_last_save = 0\n",
    "        self.model = model\n",
    "        self.path = path\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (epoch == 0):\n",
    "            with open('SC/his/' + self.path, 'w') as f:\n",
    "                # f.write(str(np.shape(HH))+'\\n')\n",
    "                f.write('Begin~' + '\\n')\n",
    "        with open('SC/his/' + self.path, 'a') as f:\n",
    "            # f.write(str(np.shape(HH))+'\\n')\n",
    "            f.write(str(logs.get('loss'))+ '\\t'+str(logs.get('val_accuracy')) + '\\n')\n",
    "       # print('SNR:',random_SNR(10))\n",
    "        if (epoch%10==0):\n",
    "            encoder1.save_weights(filepath+'F_SC_en1_'+str(epoch)+'.h5')\n",
    "            decoder1.save_weights(filepath+'F_SC_de1_'+str(epoch)+'.h5')\n",
    "            Qmodel1.save_weights(filepath+'F_Q_model1_'+str(epoch)+'.h5')\n",
    "            deQmodel1.save_weights(filepath+'F_de_Q_model1_'+str(epoch)+'.h5')\n",
    "epochs = 100  # SC_en+SC_de step1\n",
    "SC_en_Q_deQ_SC_de1.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "log=logging(SC_en_Q_deQ_SC_de1)\n",
    "\n",
    "SC_en_Q_deQ_SC_de1.fit(X,X,batch_size=128, epochs=epochs, validation_data=(Y,Y),callbacks=[log])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2ef3eb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input=keras.Input(shape=(30,), dtype=\"int64\")\n",
    "test_bits0=Qmodel0(encoder0(test_input))\n",
    "test_bits1=Qmodel1(encoder1(test_input))\n",
    "test_rx_bits0=BSC(0.05)(test_bits0)\n",
    "test_rx_bits1=BSC(0.05)(test_bits1)\n",
    "Q_output0=DeuantizationLayer(B,num_bits)(test_rx_bits0)\n",
    "Q_output1=DeuantizationLayer(B,num_bits1)(test_rx_bits1)\n",
    "Q_output=layers.Concatenate()([Q_output0,Q_output1])\n",
    "decoder_inputs = tf.ones_like(test_input, dtype=\"int64\")\n",
    "SC_de_codeword0=deQmodel0(Q_output0)\n",
    "SC_de_codeword1=deQmodel1(Q_output)\n",
    "test_pred0=decoder0([decoder_inputs,SC_de_codeword0])\n",
    "test_pred1=decoder1([decoder_inputs,SC_de_codeword1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d01f8a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 11ms/step\n"
     ]
    }
   ],
   "source": [
    "transmission_model=keras.Model(test_input,[test_rx_bits0,test_rx_bits1])\n",
    "transmitted_bits0,transmitted_bits1=transmission_model.predict(Y[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "21839647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 1. ... 0. 1. 0.]\n",
      " [0. 1. 0. ... 0. 0. 1.]\n",
      " [1. 0. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 1. ... 1. 1. 1.]\n",
      " [0. 0. 1. ... 0. 1. 0.]\n",
      " [0. 0. 1. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(transmitted_bits0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8b30924b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_53\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_67 (InputLayer)       [(None, 500)]                0         []                            \n",
      "                                                                                                  \n",
      " input_68 (InputLayer)       [(None, 400)]                0         []                            \n",
      "                                                                                                  \n",
      " deuantization_layer_28 (De  (None, 250)                  0         ['input_67[0][0]']            \n",
      " uantizationLayer)                                                                                \n",
      "                                                                                                  \n",
      " deuantization_layer_29 (De  (None, 200)                  0         ['input_68[0][0]']            \n",
      " uantizationLayer)                                                                                \n",
      "                                                                                                  \n",
      " concatenate_14 (Concatenat  (None, 450)                  0         ['deuantization_layer_28[2][0]\n",
      " e)                                                                 ',                            \n",
      "                                                                     'deuantization_layer_29[2][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " input_69 (InputLayer)       [(None, 30)]                 0         []                            \n",
      "                                                                                                  \n",
      " model_39 (Functional)       (None, 250)                  62750     ['deuantization_layer_28[2][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " model_40 (Functional)       (None, 250)                  112750    ['concatenate_14[2][0]']      \n",
      "                                                                                                  \n",
      " model_11 (Functional)       (None, 30, 5000)             5526664   ['input_69[0][0]',            \n",
      "                                                                     'model_39[6][0]']            \n",
      "                                                                                                  \n",
      " model_13 (Functional)       (None, 30, 5000)             5526664   ['input_69[0][0]',            \n",
      "                                                                     'model_40[6][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 11228828 (42.83 MB)\n",
      "Trainable params: 5639414 (21.51 MB)\n",
      "Non-trainable params: 5589414 (21.32 MB)\n",
      "__________________________________________________________________________________________________\n",
      "4/4 [==============================] - 2s 16ms/step\n"
     ]
    }
   ],
   "source": [
    "recevied_model=keras.Model([test_rx_bits0,test_rx_bits1,decoder_inputs],[test_pred0,test_pred1])\n",
    "recevied_model.summary()\n",
    "rx1,rx2=recevied_model.predict([transmitted_bits0,transmitted_bits1,np.ones([100,30])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "752b42a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rx1_result=np.argmax(rx1,-1)\n",
    "rx2_result=np.argmax(rx2,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9abb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=[]\n",
    "for i in range(100): #100transmission\n",
    "    if tokenizer.tokens_to_string(rx1_result[i])==tokenizer.tokens_to_string(Y[i]): #####should be repplaced by CRC32\n",
    "        result.append(tokenizer.tokens_to_string(rx1_result[i]))\n",
    "    else:\n",
    "        result.append(tokenizer.tokens_to_string(rx2_result[i]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf213",
   "language": "python",
   "name": "tf213"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
