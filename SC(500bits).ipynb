{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a645186b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_35 (InputLayer)       [(None, 30)]              0         \n",
      "                                                                 \n",
      " positional_embedding_14 (P  (None, 30, 128)           643840    \n",
      " ositionalEmbedding)                                             \n",
      "                                                                 \n",
      " transformer_block_33 (Tran  (None, 30, 128)           561024    \n",
      " sformerBlock)                                                   \n",
      "                                                                 \n",
      " transformer_block_34 (Tran  (None, 30, 128)           561024    \n",
      " sformerBlock)                                                   \n",
      "                                                                 \n",
      " transformer_block_35 (Tran  (None, 30, 128)           561024    \n",
      " sformerBlock)                                                   \n",
      "                                                                 \n",
      " flatten_10 (Flatten)        (None, 3840)              0         \n",
      "                                                                 \n",
      " dense_105 (Dense)           (None, 250)               960250    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3287162 (12.54 MB)\n",
      "Trainable params: 3287162 (12.54 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"model_15\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_37 (InputLayer)       [(None, 250)]                0         []                            \n",
      "                                                                                                  \n",
      " tf.reshape_6 (TFOpLambda)   (None, 250)                  0         ['input_37[0][0]']            \n",
      "                                                                                                  \n",
      " dense_106 (Dense)           (None, 3840)                 963840    ['tf.reshape_6[0][0]']        \n",
      "                                                                                                  \n",
      " input_36 (InputLayer)       [(None, 30)]                 0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_99 (La  (None, 3840)                 7680      ['dense_106[0][0]']           \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " positional_embedding_15 (P  (None, 30, 128)              643840    ['input_36[0][0]']            \n",
      " ositionalEmbedding)                                                                              \n",
      "                                                                                                  \n",
      " reshape_6 (Reshape)         (None, 30, 128)              0         ['layer_normalization_99[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " transformer_decoder_9 (Tra  (None, 30, 128)              1088768   ['positional_embedding_15[0][0\n",
      " nsformerDecoder)                                                   ]',                           \n",
      "                                                                     'reshape_6[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_81 (Dropout)        (None, 30, 128)              0         ['transformer_decoder_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " transformer_decoder_10 (Tr  (None, 30, 128)              1088768   ['dropout_81[0][0]',          \n",
      " ansformerDecoder)                                                   'reshape_6[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_82 (Dropout)        (None, 30, 128)              0         ['transformer_decoder_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " transformer_decoder_11 (Tr  (None, 30, 128)              1088768   ['dropout_82[0][0]',          \n",
      " ansformerDecoder)                                                   'reshape_6[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_83 (Dropout)        (None, 30, 128)              0         ['transformer_decoder_11[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_113 (Dense)           (None, 30, 5000)             645000    ['dropout_83[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5526664 (21.08 MB)\n",
      "Trainable params: 5526664 (21.08 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"sequence_length\": self.sequence_length,\n",
    "                \"vocab_size\": self.vocab_size,\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim,  num_heads,ff_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = ff_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim,\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.add = layers.Add()  # instead of `+` to preserve mask\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs):\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs, value=inputs, key=inputs, use_causal_mask=True\n",
    "        )\n",
    "        out_1 = self.layernorm_1(self.add([inputs, attention_output_1]))\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(self.add([out_1, attention_output_2]))\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(self.add([out_2, proj_output]))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "                \"latent_dim\": self.latent_dim,\n",
    "                \"num_heads\": self.num_heads,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "maxlen=30\n",
    "embed_dim = 128  # Embedding size for each token\n",
    "num_heads = 8  # Number of attention heads\n",
    "ff_dim = 128  # Hidden layer size in feed forward network inside transformer\n",
    "block_num=3 # block_num=6 in the paper\n",
    "vocab_size=5000\n",
    "num_bits=500\n",
    "B=2\n",
    "def SC_encoder(inputs,flat_flag=True):\n",
    "    embedding_layer = PositionalEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "   \n",
    "    for i in range(block_num):\n",
    "        x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)\n",
    "    if flat_flag:\n",
    "        x=layers.Flatten()(x)\n",
    "    x=layers.Dense(num_bits//B,activation='sigmoid')(x)\n",
    "    return x\n",
    "def SC_decoder(decoder_inputs,encoded_seq_inputs,flat_flag=True): \n",
    "    y=tf.reshape(encoded_seq_inputs,[-1,num_bits//B])\n",
    "    y=layers.Dense(embed_dim*maxlen)(y)\n",
    "    y=layers.LayerNormalization(epsilon=1e-6)(y)\n",
    "    y=layers.Reshape((maxlen, embed_dim))(y)\n",
    "    x = PositionalEmbedding(maxlen, vocab_size, embed_dim)(decoder_inputs)\n",
    "    for i in range(block_num):\n",
    "        x = TransformerDecoder(embed_dim, num_heads, ff_dim)(x, y)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "    decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "    return decoder_outputs\n",
    "encoder_inputs = keras.Input(shape=(maxlen), dtype=\"int64\")\n",
    "encoder_outputs = SC_encoder(encoder_inputs)\n",
    "encoder0 = keras.Model(encoder_inputs, encoder_outputs)\n",
    "encoder0.summary()\n",
    "decoder_inputs = keras.Input(shape=(maxlen), dtype=\"int64\")\n",
    "encoded_seq_inputs = keras.Input(shape=( num_bits//B))\n",
    "decoder_outputs = SC_decoder(decoder_inputs, encoded_seq_inputs)\n",
    "decoder0 = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
    "decoder0.summary() \n",
    "decoder_outputs = decoder0([decoder_inputs, encoder_outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9d9a223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(667875, 30)\n",
      "None\n",
      "Model: \"SC_en_de\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_38 (InputLayer)       [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " tf.ones_like_1 (TFOpLambda  (None, None)                 0         ['input_38[0][0]']            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " model_14 (Functional)       (None, 250)                  3287162   ['input_38[0][0]']            \n",
      "                                                                                                  \n",
      " model_15 (Functional)       (None, 30, 5000)             5526664   ['tf.ones_like_1[0][0]',      \n",
      "                                                                     'model_14[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8813826 (33.62 MB)\n",
      "Trainable params: 8813826 (33.62 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from europarl import TokenizerWrap\n",
    "with open('tokenizer_5000_1.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "encoder_input_data=tokenizer.tokens_padded\n",
    "print(print(encoder_input_data.shape))\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "encoder_outputs=encoder0(encoder_inputs)\n",
    "decoder_inputs = tf.ones_like(encoder_inputs, dtype=\"int64\")\n",
    "decoder_outputs=decoder0([decoder_inputs,encoder_outputs])\n",
    "SC_en_de = keras.Model(\n",
    "    encoder_inputs, decoder_outputs, name=\"SC_en_de\"\n",
    ")\n",
    "SC_en_de.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6071f411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3907/3907 [==============================] - 335s 80ms/step - loss: 3.4969 - accuracy: 0.5038 - val_loss: 2.9381 - val_accuracy: 0.5432\n",
      "Epoch 2/100\n",
      "3907/3907 [==============================] - 288s 74ms/step - loss: 2.7170 - accuracy: 0.5693 - val_loss: 2.2220 - val_accuracy: 0.6322\n",
      "Epoch 3/100\n",
      "3907/3907 [==============================] - 288s 74ms/step - loss: 2.1041 - accuracy: 0.6461 - val_loss: 1.6970 - val_accuracy: 0.7064\n",
      "Epoch 4/100\n",
      "3907/3907 [==============================] - 287s 73ms/step - loss: 1.6442 - accuracy: 0.7108 - val_loss: 1.3022 - val_accuracy: 0.7699\n",
      "Epoch 5/100\n",
      "3907/3907 [==============================] - 285s 73ms/step - loss: 1.3008 - accuracy: 0.7677 - val_loss: 1.0262 - val_accuracy: 0.8203\n",
      "Epoch 6/100\n",
      "3907/3907 [==============================] - 286s 73ms/step - loss: 1.0340 - accuracy: 0.8126 - val_loss: 0.7676 - val_accuracy: 0.8635\n",
      "Epoch 7/100\n",
      "3907/3907 [==============================] - 287s 74ms/step - loss: 0.8085 - accuracy: 0.8523 - val_loss: 0.5744 - val_accuracy: 0.9007\n",
      "Epoch 8/100\n",
      "3907/3907 [==============================] - 286s 73ms/step - loss: 0.6232 - accuracy: 0.8884 - val_loss: 0.4625 - val_accuracy: 0.9184\n",
      "Epoch 9/100\n",
      "3907/3907 [==============================] - 283s 73ms/step - loss: 0.5186 - accuracy: 0.9053 - val_loss: 0.3910 - val_accuracy: 0.9273\n",
      "Epoch 10/100\n",
      "3907/3907 [==============================] - 286s 73ms/step - loss: 0.4509 - accuracy: 0.9150 - val_loss: 0.3480 - val_accuracy: 0.9330\n",
      "Epoch 11/100\n",
      "3907/3907 [==============================] - 285s 73ms/step - loss: 0.3774 - accuracy: 0.9289 - val_loss: 0.2802 - val_accuracy: 0.9484\n",
      "Epoch 12/100\n",
      "3907/3907 [==============================] - 286s 73ms/step - loss: 0.3312 - accuracy: 0.9380 - val_loss: 0.2528 - val_accuracy: 0.9529\n",
      "Epoch 13/100\n",
      "3907/3907 [==============================] - 285s 73ms/step - loss: 0.2999 - accuracy: 0.9432 - val_loss: 0.2109 - val_accuracy: 0.9592\n",
      "Epoch 14/100\n",
      "3907/3907 [==============================] - 285s 73ms/step - loss: 0.2737 - accuracy: 0.9467 - val_loss: 0.2160 - val_accuracy: 0.9575\n",
      "Epoch 15/100\n",
      "3907/3907 [==============================] - 285s 73ms/step - loss: 0.2768 - accuracy: 0.9462 - val_loss: 0.1981 - val_accuracy: 0.9619\n",
      "Epoch 16/100\n",
      "3907/3907 [==============================] - 286s 73ms/step - loss: 0.2687 - accuracy: 0.9472 - val_loss: 0.2078 - val_accuracy: 0.9592\n",
      "Epoch 17/100\n",
      "3907/3907 [==============================] - 286s 73ms/step - loss: 0.2311 - accuracy: 0.9550 - val_loss: 0.1800 - val_accuracy: 0.9655\n",
      "Epoch 18/100\n",
      "3907/3907 [==============================] - 286s 73ms/step - loss: 0.2214 - accuracy: 0.9572 - val_loss: 0.1791 - val_accuracy: 0.9661\n",
      "Epoch 19/100\n",
      "3907/3907 [==============================] - 285s 73ms/step - loss: 0.2201 - accuracy: 0.9574 - val_loss: 0.1746 - val_accuracy: 0.9665\n",
      "Epoch 20/100\n",
      "3907/3907 [==============================] - 286s 73ms/step - loss: 0.2188 - accuracy: 0.9571 - val_loss: 0.1754 - val_accuracy: 0.9665\n",
      "Epoch 21/100\n",
      "3907/3907 [==============================] - 287s 73ms/step - loss: 0.2035 - accuracy: 0.9603 - val_loss: 0.1704 - val_accuracy: 0.9674\n",
      "Epoch 22/100\n",
      "3907/3907 [==============================] - 285s 73ms/step - loss: 0.2013 - accuracy: 0.9605 - val_loss: 0.1676 - val_accuracy: 0.9682\n",
      "Epoch 23/100\n",
      "3907/3907 [==============================] - 286s 73ms/step - loss: 0.2260 - accuracy: 0.9553 - val_loss: 0.1573 - val_accuracy: 0.9700\n",
      "Epoch 24/100\n",
      "3907/3907 [==============================] - 286s 73ms/step - loss: 0.1918 - accuracy: 0.9621 - val_loss: 0.1514 - val_accuracy: 0.9712\n",
      "Epoch 25/100\n",
      "3907/3907 [==============================] - 285s 73ms/step - loss: 0.1831 - accuracy: 0.9639 - val_loss: 0.1440 - val_accuracy: 0.9725\n",
      "Epoch 26/100\n",
      "3907/3907 [==============================] - 286s 73ms/step - loss: 0.1707 - accuracy: 0.9660 - val_loss: 0.1280 - val_accuracy: 0.9756\n",
      "Epoch 27/100\n",
      "3907/3907 [==============================] - 286s 73ms/step - loss: 0.1532 - accuracy: 0.9690 - val_loss: 0.1191 - val_accuracy: 0.9769\n",
      "Epoch 28/100\n",
      "3907/3907 [==============================] - 285s 73ms/step - loss: 0.1444 - accuracy: 0.9710 - val_loss: 0.1083 - val_accuracy: 0.9794\n",
      "Epoch 29/100\n",
      "3907/3907 [==============================] - 286s 73ms/step - loss: 0.1293 - accuracy: 0.9738 - val_loss: 0.1082 - val_accuracy: 0.9790\n",
      "Epoch 30/100\n",
      "3907/3907 [==============================] - 286s 73ms/step - loss: 0.1350 - accuracy: 0.9727 - val_loss: 0.6745 - val_accuracy: 0.8730\n",
      "Epoch 31/100\n",
      "3907/3907 [==============================] - 286s 73ms/step - loss: 0.1251 - accuracy: 0.9743 - val_loss: 0.0920 - val_accuracy: 0.9819\n",
      "Epoch 32/100\n",
      "3907/3907 [==============================] - 285s 73ms/step - loss: 0.1095 - accuracy: 0.9773 - val_loss: 0.0926 - val_accuracy: 0.9820\n",
      "Epoch 33/100\n",
      "3907/3907 [==============================] - 286s 73ms/step - loss: 0.1035 - accuracy: 0.9784 - val_loss: 0.0926 - val_accuracy: 0.9815\n",
      "Epoch 34/100\n",
      "3907/3907 [==============================] - 286s 73ms/step - loss: 0.1157 - accuracy: 0.9765 - val_loss: 0.0991 - val_accuracy: 0.9802\n",
      "Epoch 35/100\n",
      "3907/3907 [==============================] - 286s 73ms/step - loss: 0.1268 - accuracy: 0.9740 - val_loss: 0.0915 - val_accuracy: 0.9822\n",
      "Epoch 36/100\n",
      "3907/3907 [==============================] - 286s 73ms/step - loss: 0.1126 - accuracy: 0.9774 - val_loss: 0.0751 - val_accuracy: 0.9853\n",
      "Epoch 37/100\n",
      "3907/3907 [==============================] - 284s 73ms/step - loss: 0.1037 - accuracy: 0.9792 - val_loss: 0.0712 - val_accuracy: 0.9862\n",
      "Epoch 38/100\n",
      "3907/3907 [==============================] - 286s 73ms/step - loss: 0.0867 - accuracy: 0.9817 - val_loss: 0.0719 - val_accuracy: 0.9859\n",
      "Epoch 39/100\n",
      "3907/3907 [==============================] - 285s 73ms/step - loss: 0.0849 - accuracy: 0.9822 - val_loss: 0.0738 - val_accuracy: 0.9858\n",
      "Epoch 40/100\n",
      "3907/3907 [==============================] - 285s 73ms/step - loss: 0.0860 - accuracy: 0.9821 - val_loss: 0.0668 - val_accuracy: 0.9869\n",
      "Epoch 41/100\n",
      "3907/3907 [==============================] - 285s 73ms/step - loss: 0.0902 - accuracy: 0.9814 - val_loss: 0.0738 - val_accuracy: 0.9852\n",
      "Epoch 42/100\n",
      "3907/3907 [==============================] - 286s 73ms/step - loss: 0.0872 - accuracy: 0.9812 - val_loss: 0.0623 - val_accuracy: 0.9878\n",
      "Epoch 43/100\n",
      "3907/3907 [==============================] - 285s 73ms/step - loss: 0.0811 - accuracy: 0.9827 - val_loss: 0.0609 - val_accuracy: 0.9881\n",
      "Epoch 44/100\n",
      "3907/3907 [==============================] - 286s 73ms/step - loss: 0.0745 - accuracy: 0.9839 - val_loss: 0.0641 - val_accuracy: 0.9869\n",
      "Epoch 45/100\n",
      "3907/3907 [==============================] - 285s 73ms/step - loss: 0.0855 - accuracy: 0.9826 - val_loss: 0.0614 - val_accuracy: 0.9882\n",
      "Epoch 46/100\n",
      "3907/3907 [==============================] - 285s 73ms/step - loss: 0.0813 - accuracy: 0.9832 - val_loss: 0.0587 - val_accuracy: 0.9886\n",
      "Epoch 47/100\n",
      "3907/3907 [==============================] - 285s 73ms/step - loss: 0.0710 - accuracy: 0.9848 - val_loss: 0.0577 - val_accuracy: 0.9887\n",
      "Epoch 48/100\n",
      "3907/3907 [==============================] - 286s 73ms/step - loss: 0.0799 - accuracy: 0.9835 - val_loss: 0.0809 - val_accuracy: 0.9850\n",
      "Epoch 49/100\n",
      "3907/3907 [==============================] - 285s 73ms/step - loss: 0.0839 - accuracy: 0.9831 - val_loss: 0.0622 - val_accuracy: 0.9882\n",
      "Epoch 50/100\n",
      "3907/3907 [==============================] - 286s 73ms/step - loss: 0.0915 - accuracy: 0.9823 - val_loss: 0.1786 - val_accuracy: 0.9691\n",
      "Epoch 51/100\n",
      "3907/3907 [==============================] - 286s 73ms/step - loss: 0.0891 - accuracy: 0.9812 - val_loss: 0.0627 - val_accuracy: 0.9871\n",
      "Epoch 52/100\n",
      "3907/3907 [==============================] - 287s 73ms/step - loss: 0.0677 - accuracy: 0.9857 - val_loss: 0.0832 - val_accuracy: 0.9853\n",
      "Epoch 53/100\n",
      "3907/3907 [==============================] - 286s 73ms/step - loss: 0.0872 - accuracy: 0.9831 - val_loss: 0.0640 - val_accuracy: 0.9893\n",
      "Epoch 54/100\n",
      "3907/3907 [==============================] - 286s 73ms/step - loss: 0.0736 - accuracy: 0.9859 - val_loss: 0.0618 - val_accuracy: 0.9893\n",
      "Epoch 55/100\n",
      "3907/3907 [==============================] - 286s 73ms/step - loss: 0.0682 - accuracy: 0.9867 - val_loss: 0.0572 - val_accuracy: 0.9902\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3907/3907 [==============================] - 285s 73ms/step - loss: 0.0629 - accuracy: 0.9874 - val_loss: 0.0524 - val_accuracy: 0.9905\n",
      "Epoch 57/100\n",
      "3907/3907 [==============================] - 286s 73ms/step - loss: 0.0600 - accuracy: 0.9877 - val_loss: 0.0489 - val_accuracy: 0.9911\n",
      "Epoch 58/100\n",
      "3907/3907 [==============================] - 286s 73ms/step - loss: 0.0710 - accuracy: 0.9860 - val_loss: 0.0774 - val_accuracy: 0.9860\n",
      "Epoch 59/100\n",
      "3907/3907 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9857"
     ]
    }
   ],
   "source": [
    "import os\n",
    "X = tokenizer.tokens_padded[0:500000]\n",
    "Y = tokenizer.tokens_padded[500000:]\n",
    "filepath ='SC/'\n",
    "if not os.path.exists(filepath):\n",
    "                os.mkdir(filepath[:-1])\n",
    "if not os.path.exists(filepath+'/his'):\n",
    "                os.mkdir(filepath)\n",
    "class logging(keras.callbacks.Callback):\n",
    "    def __init__(self, model, path='SC.txt'):\n",
    "        super(logging, self).__init__()\n",
    "\n",
    "        self.epochs_since_last_save = 0\n",
    "        self.model = model\n",
    "        self.path = path\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (epoch == 0):\n",
    "            with open('SC/his/' + self.path, 'w') as f:\n",
    "                # f.write(str(np.shape(HH))+'\\n')\n",
    "                f.write('Begin~' + '\\n')\n",
    "        with open('SC/his/' + self.path, 'a') as f:\n",
    "            # f.write(str(np.shape(HH))+'\\n')\n",
    "            f.write(str(logs.get('loss'))+ '\\t'+str(logs.get('val_accuracy')) + '\\n')\n",
    "       # print('SNR:',random_SNR(10))\n",
    "        if (epoch%10==0):\n",
    "            encoder0.save_weights(filepath+'SC_en_'+str(epoch)+'.h5')\n",
    "            decoder0.save_weights(filepath+'SC_de_'+str(epoch)+'.h5')\n",
    "epochs = 100  # SC_en+SC_de step1\n",
    "SC_en_de.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "log=logging(SC_en_de)\n",
    "SC_en_de.fit(X,X,batch_size=128, epochs=epochs, validation_data=(Y,Y),callbacks=[log])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "97ffbcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder0.save_weights(filepath+'SC_en0.h5')\n",
    "# decoder0.save_weights(filepath+'SC_de0.h5')  \n",
    "encoder0.load_weights(filepath+'SC_en0.h5')\n",
    "decoder0.load_weights(filepath+'SC_de0.h5') \n",
    "encoder0.trainable=False\n",
    "decoder0.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7ed47ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100/5247 [..............................] - ETA: 1:12 - loss: 0.0368 - accuracy: 0.9915"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[162], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mSC_en_de\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf213/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf213/lib/python3.9/site-packages/keras/src/engine/training.py:2200\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   2196\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   2197\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, step_num\u001b[38;5;241m=\u001b[39mstep, _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2198\u001b[0m             ):\n\u001b[1;32m   2199\u001b[0m                 callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[0;32m-> 2200\u001b[0m                 logs \u001b[38;5;241m=\u001b[39m \u001b[43mtest_function_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2201\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdataset_or_iterator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2202\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdata_handler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2203\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2204\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pss_evaluation_shards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2205\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2207\u001b[0m logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[1;32m   2208\u001b[0m \u001b[38;5;66;03m# Override with model metrics instead of last step logs\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf213/lib/python3.9/site-packages/keras/src/engine/training.py:4000\u001b[0m, in \u001b[0;36m_TestFunction.run_step\u001b[0;34m(self, dataset_or_iterator, data_handler, step, unused_shards)\u001b[0m\n\u001b[1;32m   3999\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset_or_iterator, data_handler, step, unused_shards):\n\u001b[0;32m-> 4000\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_or_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4001\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   4002\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf213/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf213/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf213/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:864\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    862\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 864\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf213/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf213/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf213/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/anaconda3/envs/tf213/lib/python3.9/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/envs/tf213/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "SC_en_de.evaluate(Y,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "11e83a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "B=2\n",
    "N=16\n",
    "d_model=128\n",
    "NUM_FEEDBACK_BITS=500\n",
    "def Num2Bit(Num, B):\n",
    "    Num_ = Num.numpy()\n",
    "    bit = (np.unpackbits(np.array(Num_, np.uint8), axis=1).reshape(-1, Num_.shape[1], 8)[:, :, (8-B):]).reshape(-1,\n",
    "                                                                                                            Num_.shape[\n",
    "                                                                                                                1] * B)\n",
    "    bit.astype(np.float32)\n",
    "    return tf.convert_to_tensor(bit, dtype=tf.float32)\n",
    "# Bit to Number Function Defining\n",
    "def Bit2Num(Bit, B):\n",
    "    Bit_ = Bit.numpy()\n",
    "    Bit_.astype(np.float32)\n",
    "    Bit_ = np.reshape(Bit_, [-1, int(Bit_.shape[1] / B), B])\n",
    "    num = np.zeros(shape=np.shape(Bit_[:, :, 1]))\n",
    "    for i in range(B):\n",
    "        num = num + Bit_[:, :, i] * 2 ** (B - 1 - i)\n",
    "    return tf.cast(num, dtype=tf.float32)\n",
    "#=======================================================================================================================\n",
    "#=======================================================================================================================\n",
    "# Quantization and Dequantization Layers Defining\n",
    "@tf.custom_gradient\n",
    "def QuantizationOp(x, B):\n",
    "    step = tf.cast((2 ** B), dtype=tf.float32)\n",
    "    result = tf.cast((tf.round(x * step - 0.5)), dtype=tf.float32)\n",
    "    dim = result.shape[1]\n",
    "    result = tf.py_function(func=Num2Bit, inp=[result, B], Tout=tf.float32)\n",
    "    result = tf.reshape(result, [-1, NUM_FEEDBACK_BITS])\n",
    "    def custom_grad(dy):\n",
    "        grad = dy\n",
    "        return (grad, grad)\n",
    "    return result, custom_grad\n",
    "class QuantizationLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, B,**kwargs):\n",
    "        self.B = B\n",
    "        super(QuantizationLayer, self).__init__()\n",
    "    def call(self, x):\n",
    "        return QuantizationOp(x, self.B)\n",
    "    def get_config(self):\n",
    "        # Implement get_config to enable serialization. This is optional.\n",
    "        base_config = super(QuantizationLayer, self).get_config()\n",
    "        base_config['B'] = self.B\n",
    "        return base_config\n",
    "@tf.custom_gradient\n",
    "def DequantizationOp(x, B):\n",
    "    dim = x.shape[1]\n",
    "    x = tf.py_function(func=Bit2Num, inp=[x, B], Tout=tf.float32)\n",
    "    x = tf.reshape(x, (-1, NUM_FEEDBACK_BITS//B))\n",
    "    step = tf.cast((2 ** B), dtype=tf.float32)\n",
    "    result = tf.cast((x + 0.5) / step, dtype=tf.float32)\n",
    "    def custom_grad(dy):\n",
    "        grad = dy * 1\n",
    "        return (grad, grad)\n",
    "    return result, custom_grad\n",
    "class DeuantizationLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, B,**kwargs):\n",
    "        self.B = B\n",
    "        super(DeuantizationLayer, self).__init__()\n",
    "    def call(self, x):\n",
    "        return DequantizationOp(x, self.B)\n",
    "    def get_config(self):\n",
    "        base_config = super(DeuantizationLayer, self).get_config()\n",
    "        base_config['B'] = self.B\n",
    "        return base_config\n",
    "@tf.custom_gradient\n",
    "def BSCOp(bits, rate):\n",
    "    errorbits = tf.keras.backend.random_binomial(shape=tf.shape(bits), p=rate)\n",
    "    result=tf.reshape(tf.math.floormod(bits+errorbits,2),tf.shape(bits))\n",
    "    def custom_grad(dy):\n",
    "        grad = dy * 1\n",
    "        return (grad, grad)\n",
    "    return result, custom_grad\n",
    "class BSC(tf.keras.layers.Layer):\n",
    "    def __init__(self, rate, **kwargs): #**kwargs```TypeError: __init__() got an unexpected keyword argument 'name'```\n",
    "        self.rate = rate\n",
    "        super(BSC, self).__init__()\n",
    "    def call(self, bits):\n",
    "        \n",
    "        return BSCOp(bits,self.rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d8cb8be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input=keras.Input(shape=(30,))\n",
    "a=BSC(0.5)(test_input)\n",
    "test_model=keras.Model(test_input,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "bccb0256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 65ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
       "        0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0.]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model.predict(np.ones([1,30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "70277306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15625/15625 [==============================] - 41s 3ms/step\n",
      "5247/5247 [==============================] - 24s 5ms/step\n",
      "(500000, 250)\n"
     ]
    }
   ],
   "source": [
    "X_codeword=encoder0.predict(X)\n",
    "Y_codeword=encoder0.predict(Y)\n",
    "print(X_codeword.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "4edd6c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_43\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_99 (InputLayer)       [(None, 250)]             0         \n",
      "                                                                 \n",
      " dense_126 (Dense)           (None, 250)               62750     \n",
      "                                                                 \n",
      " quantization_layer_24 (Qua  (None, 500)               0         \n",
      " ntizationLayer)                                                 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 62750 (245.12 KB)\n",
      "Trainable params: 62750 (245.12 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"model_44\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_100 (InputLayer)      [(None, 500)]             0         \n",
      "                                                                 \n",
      " deuantization_layer_23 (De  (None, 250)               0         \n",
      " uantizationLayer)                                               \n",
      "                                                                 \n",
      " dense_127 (Dense)           (None, 250)               62750     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 62750 (245.12 KB)\n",
      "Trainable params: 62750 (245.12 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"Q_deQ\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_101 (InputLayer)      [(None, 250)]             0         \n",
      "                                                                 \n",
      " dense_126 (Dense)           (None, 250)               62750     \n",
      "                                                                 \n",
      " quantization_layer_24 (Qua  (None, 500)               0         \n",
      " ntizationLayer)                                                 \n",
      "                                                                 \n",
      " bsc_28 (BSC)                (None, 500)               0         \n",
      "                                                                 \n",
      " deuantization_layer_23 (De  (None, 250)               0         \n",
      " uantizationLayer)                                               \n",
      "                                                                 \n",
      " dense_127 (Dense)           (None, 250)               62750     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 125500 (490.23 KB)\n",
      "Trainable params: 125500 (490.23 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"SC_en_Q_deQ_SC_de\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_98 (InputLayer)       [(None, 30)]                 0         []                            \n",
      "                                                                                                  \n",
      " model_14 (Functional)       (None, 250)                  3287162   ['input_98[0][0]']            \n",
      "                                                                                                  \n",
      " dense_126 (Dense)           (None, 250)                  62750     ['model_14[25][0]']           \n",
      "                                                                                                  \n",
      " quantization_layer_24 (Qua  (None, 500)                  0         ['dense_126[0][0]']           \n",
      " ntizationLayer)                                                                                  \n",
      "                                                                                                  \n",
      " bsc_28 (BSC)                (None, 500)                  0         ['quantization_layer_24[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " deuantization_layer_23 (De  (None, 250)                  0         ['bsc_28[0][0]']              \n",
      " uantizationLayer)                                                                                \n",
      "                                                                                                  \n",
      " tf.ones_like_25 (TFOpLambd  (None, 30)                   0         ['input_98[0][0]']            \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " dense_127 (Dense)           (None, 250)                  62750     ['deuantization_layer_23[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " model_15 (Functional)       (None, 30, 5000)             5526664   ['tf.ones_like_25[0][0]',     \n",
      "                                                                     'dense_127[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8939326 (34.10 MB)\n",
      "Trainable params: 125500 (490.23 KB)\n",
      "Non-trainable params: 8813826 (33.62 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = keras.Input(shape=(30,), dtype=\"int64\")\n",
    "SC_en_codeword = encoder0(encoder_inputs)\n",
    "Q_input=layers.Dense(NUM_FEEDBACK_BITS//B,activation='sigmoid')(SC_en_codeword)\n",
    "bits=QuantizationLayer(B)(Q_input)#transmission bits\n",
    "Qmodel=keras.Model(SC_en_codeword,bits)\n",
    "Qmodel.summary()\n",
    "rx_bits=BSC(0.0)(bits)\n",
    "Q_output=DeuantizationLayer(B)(rx_bits)\n",
    "SC_de_codeword=layers.Dense(NUM_FEEDBACK_BITS//B,activation='sigmoid')(Q_output)\n",
    "deQmodel=keras.Model(rx_bits,SC_de_codeword)\n",
    "deQmodel.summary()\n",
    "decoder_inputs = tf.ones_like(encoder_inputs, dtype=\"int64\")\n",
    "decoder_outputs=decoder0([decoder_inputs,SC_de_codeword])\n",
    "Q_deQ = keras.Model(\n",
    "    SC_en_codeword, SC_de_codeword, name=\"Q_deQ\"\n",
    ")\n",
    "Q_deQ.summary()\n",
    "SC_en_Q_deQ_SC_de = keras.Model(\n",
    "    encoder_inputs, decoder_outputs, name=\"SC_en_Q_deQ_SC_de\"\n",
    ")\n",
    "SC_en_Q_deQ_SC_de.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bd8390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1950/3907 [=============>................] - ETA: 14s - loss: 0.0156 - mse: 0.0156"
     ]
    }
   ],
   "source": [
    "# X_codeword=encoder0.predict(X)\n",
    "# Y_codeword=encoder0.predict(Y)\n",
    "# print(X_codeword.shape)\n",
    "SC_en_Q_deQ_SC_de.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.00001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "filepath ='SC/'\n",
    "if not os.path.exists(filepath):\n",
    "                os.mkdir(filepath[:-1])\n",
    "if not os.path.exists(filepath+'/his'):\n",
    "                os.mkdir(filepath)\n",
    "class logging(keras.callbacks.Callback):\n",
    "    def __init__(self, model, path='Q_deQ.txt'):\n",
    "        super(logging, self).__init__()\n",
    "\n",
    "        self.epochs_since_last_save = 0\n",
    "        self.model = model\n",
    "        self.path = path\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (epoch == 0):\n",
    "            with open('SC/his/' + self.path, 'w') as f:\n",
    "                # f.write(str(np.shape(HH))+'\\n')\n",
    "                f.write('Begin~' + '\\n')\n",
    "        with open('SC/his/' + self.path, 'a') as f:\n",
    "            # f.write(str(np.shape(HH))+'\\n')\n",
    "            f.write(str(logs.get('loss'))+ '\\t'+str(logs.get('val_accuracy')) + '\\n')\n",
    "        SC_en_Q_deQ_SC_de.evaluate(Y[:100],Y[:100])\n",
    "        if (epoch%10==0):\n",
    "            \n",
    "            Qmodel.save_weights(filepath+'Q_model_'+str(epoch)+'.h5')\n",
    "            deQmodel.save_weights(filepath+'deQ_model_'+str(epoch)+'.h5')\n",
    "epochs = 100  # SC_en+SC_de step1\n",
    "Q_deQ.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0005), loss=\"mse\", metrics=[\"mse\"]\n",
    ")\n",
    "log=logging(Q_deQ)\n",
    "Q_deQ.fit(X_codeword,X_codeword,batch_size=128, epochs=epochs,callbacks=[log])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "ecad12fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder0.trainable=False\n",
    "# decoder0.trainable=False\n",
    "# filepath ='SC/'\n",
    "# if not os.path.exists(filepath):\n",
    "#                 os.mkdir(filepath[:-1])\n",
    "# if not os.path.exists(filepath+'/his'):\n",
    "#                 os.mkdir(filepath)\n",
    "# class logging(keras.callbacks.Callback):\n",
    "#     def __init__(self, model, path='Q_SC.txt'):\n",
    "#         super(logging, self).__init__()\n",
    "\n",
    "#         self.epochs_since_last_save = 0\n",
    "#         self.model = model\n",
    "#         self.path = path\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs={}):\n",
    "#         if (epoch == 0):\n",
    "#             with open('SC/his/' + self.path, 'w') as f:\n",
    "#                 # f.write(str(np.shape(HH))+'\\n')\n",
    "#                 f.write('Begin~' + '\\n')\n",
    "#         with open('SC/his/' + self.path, 'a') as f:\n",
    "#             # f.write(str(np.shape(HH))+'\\n')\n",
    "#             f.write(str(logs.get('loss'))+ '\\t'+str(logs.get('val_accuracy')) + '\\n')\n",
    "#        # print('SNR:',random_SNR(10))\n",
    "#         if (epoch%10==0):\n",
    "#             encoder0.save_weights(filepath+'F_SC_en_'+str(epoch)+'.h5')\n",
    "#             decoder0.save_weights(filepath+'F_SC_de_'+str(epoch)+'.h5')\n",
    "#             Qmodel.save_weights(filepath+'F_Q_model_'+str(epoch)+'.h5')\n",
    "#             deQmodel.save_weights(filepath+'F_de_Q_model_'+str(epoch)+'.h5')\n",
    "# epochs = 100  # SC_en+SC_de step1\n",
    "# SC_en_Q_deQ_SC_de.compile(\n",
    "#     optimizer=keras.optimizers.Adam(learning_rate=0.00001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "# )\n",
    "# log=logging(SC_en_Q_deQ_SC_de)\n",
    "\n",
    "# encoder0.trainable=True\n",
    "# decoder0.trainable=True\n",
    "# Qmodel.trainable=True\n",
    "# deQmodel.trainable=True\n",
    "# SC_en_Q_deQ_SC_de.summary()\n",
    "#  SC_en_Q_deQ_SC_de.fit(X,X,batch_size=128, epochs=10, validation_data=(Y,Y),callbacks=[log])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "41059e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4845 - accuracy: 0.9140\n",
      "4/4 [==============================] - 0s 17ms/step\n",
      "(100, 30)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'yet we are supposed to allow these countries into schengen'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SC_en_Q_deQ_SC_de.evaluate(Y[:100],Y[:100])\n",
    "result=np.argmax(SC_en_Q_deQ_SC_de.predict(Y[:100]),-1)\n",
    "print(result.shape)\n",
    "tokenizer.tokens_to_string(result[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf213",
   "language": "python",
   "name": "tf213"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
